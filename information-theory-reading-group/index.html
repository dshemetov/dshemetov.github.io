<!doctype html><html lang=en-us><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Dmitry's Very Online Internet Webpage Site</title><meta name=title content="Dmitry's Very Online Internet Webpage Site"><meta name=description content="Information Theory and Statistics Reading Group
This group met from Fall 2018 to Winter 2019.
Winter Quarter

[03/05/2019] Tong Yitang presents on Lecture 71. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation.
[02/26/2019] David Weber presents on Lecture 61. Topics include graphical models.
[02/19/2019] David Weber and Dmitry Shemetov present on Lecture 61. Topics include mutual information estimators and applications of mutual information to machine learning.
[02/12/2019] Dmitry Shemetov presents on Lecture 4 and 51. Topics include properties of submodular functions, maximizing submodular funcions, and an information theoretic clustering algorithm2.
[01/29/2019] Dmitry Shemetov presents on Lecture 31. Topics include the data processing inequality, submodular functions, submodularity of mutual information, maximizing submodular functions, and an application of sensor placement3.
[01/22/2019] David Weber presents on the challenges of estimating mutual information4 5.
[01/15/2019] Cong Xu presents on “Chapter 5: Data Compression”6.

Fall Quarter

[12/13/2018] Dmitry Shemetov presents an introduction to some work by Katalin Marton7: the blowing up lemma8, single-letter characterization, Ornstein’s copying lemma, and Talagrand’s theorem.
[12/07/2018] David Weber lays the foundations with “Chapter 8: Differential Entropy”6 and then presents on “Estimating Mutual Information”4.
[11/29/2018] Yiqun Shao presents9.
[11/09/2018] Stephen Sheng presents10.
[11/02/2018] Group read of “Chapter 11: Information Theory and Statistics”6.
[10/26/2018] Group read of “Chapter 6: Gambling and Data Compression”6.
[10/19/2018] Dmitry Shemetov presents on “Chapter 2: Lower bounds on minimax risk”11.
[10/12/2018] A problem solving session dedicated to Chapter 26.
[10/05/2018] Dmitry Shemetov presents the foundational material from “Chapter 2: Entropy, Relative Entropy, Mutual Information”6.
[9/28/2018] Organizing meeting.

References




A. Singh, “Data Processing and Information Theory Course Notes”&#160;&#8617;&#xfe0e;&#160;&#8617;&#xfe0e;&#160;&#8617;&#xfe0e;&#160;&#8617;&#xfe0e;&#160;&#8617;&#xfe0e;"><meta name=keywords content><meta property="og:url" content="https://dshemetov.github.io/information-theory-reading-group/"><meta property="og:site_name" content="Dmitry's Very Online Internet Webpage Site"><meta property="og:title" content="Dmitry's Very Online Internet Webpage Site"><meta property="og:description" content="Information Theory and Statistics Reading Group This group met from Fall 2018 to Winter 2019.
Winter Quarter [03/05/2019] Tong Yitang presents on Lecture 71. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation. [02/26/2019] David Weber presents on Lecture 61. Topics include graphical models. [02/19/2019] David Weber and Dmitry Shemetov present on Lecture 61. Topics include mutual information estimators and applications of mutual information to machine learning. [02/12/2019] Dmitry Shemetov presents on Lecture 4 and 51. Topics include properties of submodular functions, maximizing submodular funcions, and an information theoretic clustering algorithm2. [01/29/2019] Dmitry Shemetov presents on Lecture 31. Topics include the data processing inequality, submodular functions, submodularity of mutual information, maximizing submodular functions, and an application of sensor placement3. [01/22/2019] David Weber presents on the challenges of estimating mutual information4 5. [01/15/2019] Cong Xu presents on “Chapter 5: Data Compression”6. Fall Quarter [12/13/2018] Dmitry Shemetov presents an introduction to some work by Katalin Marton7: the blowing up lemma8, single-letter characterization, Ornstein’s copying lemma, and Talagrand’s theorem. [12/07/2018] David Weber lays the foundations with “Chapter 8: Differential Entropy”6 and then presents on “Estimating Mutual Information”4. [11/29/2018] Yiqun Shao presents9. [11/09/2018] Stephen Sheng presents10. [11/02/2018] Group read of “Chapter 11: Information Theory and Statistics”6. [10/26/2018] Group read of “Chapter 6: Gambling and Data Compression”6. [10/19/2018] Dmitry Shemetov presents on “Chapter 2: Lower bounds on minimax risk”11. [10/12/2018] A problem solving session dedicated to Chapter 26. [10/05/2018] Dmitry Shemetov presents the foundational material from “Chapter 2: Entropy, Relative Entropy, Mutual Information”6. [9/28/2018] Organizing meeting. References A. Singh, “Data Processing and Information Theory Course Notes” ↩︎ ↩︎ ↩︎ ↩︎ ↩︎"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:published_time" content="2019-08-25T00:00:00+00:00"><meta property="article:modified_time" content="2019-08-25T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Dmitry's Very Online Internet Webpage Site"><meta name=twitter:description content="Information Theory and Statistics Reading Group This group met from Fall 2018 to Winter 2019.
Winter Quarter [03/05/2019] Tong Yitang presents on Lecture 71. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation. [02/26/2019] David Weber presents on Lecture 61. Topics include graphical models. [02/19/2019] David Weber and Dmitry Shemetov present on Lecture 61. Topics include mutual information estimators and applications of mutual information to machine learning. [02/12/2019] Dmitry Shemetov presents on Lecture 4 and 51. Topics include properties of submodular functions, maximizing submodular funcions, and an information theoretic clustering algorithm2. [01/29/2019] Dmitry Shemetov presents on Lecture 31. Topics include the data processing inequality, submodular functions, submodularity of mutual information, maximizing submodular functions, and an application of sensor placement3. [01/22/2019] David Weber presents on the challenges of estimating mutual information4 5. [01/15/2019] Cong Xu presents on “Chapter 5: Data Compression”6. Fall Quarter [12/13/2018] Dmitry Shemetov presents an introduction to some work by Katalin Marton7: the blowing up lemma8, single-letter characterization, Ornstein’s copying lemma, and Talagrand’s theorem. [12/07/2018] David Weber lays the foundations with “Chapter 8: Differential Entropy”6 and then presents on “Estimating Mutual Information”4. [11/29/2018] Yiqun Shao presents9. [11/09/2018] Stephen Sheng presents10. [11/02/2018] Group read of “Chapter 11: Information Theory and Statistics”6. [10/26/2018] Group read of “Chapter 6: Gambling and Data Compression”6. [10/19/2018] Dmitry Shemetov presents on “Chapter 2: Lower bounds on minimax risk”11. [10/12/2018] A problem solving session dedicated to Chapter 26. [10/05/2018] Dmitry Shemetov presents the foundational material from “Chapter 2: Entropy, Relative Entropy, Mutual Information”6. [9/28/2018] Organizing meeting. References A. Singh, “Data Processing and Information Theory Course Notes” ↩︎ ↩︎ ↩︎ ↩︎ ↩︎"><meta itemprop=name content="Dmitry's Very Online Internet Webpage Site"><meta itemprop=description content="Information Theory and Statistics Reading Group This group met from Fall 2018 to Winter 2019.
Winter Quarter [03/05/2019] Tong Yitang presents on Lecture 71. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation. [02/26/2019] David Weber presents on Lecture 61. Topics include graphical models. [02/19/2019] David Weber and Dmitry Shemetov present on Lecture 61. Topics include mutual information estimators and applications of mutual information to machine learning. [02/12/2019] Dmitry Shemetov presents on Lecture 4 and 51. Topics include properties of submodular functions, maximizing submodular funcions, and an information theoretic clustering algorithm2. [01/29/2019] Dmitry Shemetov presents on Lecture 31. Topics include the data processing inequality, submodular functions, submodularity of mutual information, maximizing submodular functions, and an application of sensor placement3. [01/22/2019] David Weber presents on the challenges of estimating mutual information4 5. [01/15/2019] Cong Xu presents on “Chapter 5: Data Compression”6. Fall Quarter [12/13/2018] Dmitry Shemetov presents an introduction to some work by Katalin Marton7: the blowing up lemma8, single-letter characterization, Ornstein’s copying lemma, and Talagrand’s theorem. [12/07/2018] David Weber lays the foundations with “Chapter 8: Differential Entropy”6 and then presents on “Estimating Mutual Information”4. [11/29/2018] Yiqun Shao presents9. [11/09/2018] Stephen Sheng presents10. [11/02/2018] Group read of “Chapter 11: Information Theory and Statistics”6. [10/26/2018] Group read of “Chapter 6: Gambling and Data Compression”6. [10/19/2018] Dmitry Shemetov presents on “Chapter 2: Lower bounds on minimax risk”11. [10/12/2018] A problem solving session dedicated to Chapter 26. [10/05/2018] Dmitry Shemetov presents the foundational material from “Chapter 2: Entropy, Relative Entropy, Mutual Information”6. [9/28/2018] Organizing meeting. References A. Singh, “Data Processing and Information Theory Course Notes” ↩︎ ↩︎ ↩︎ ↩︎ ↩︎"><meta itemprop=datePublished content="2019-08-25T00:00:00+00:00"><meta itemprop=dateModified content="2019-08-25T00:00:00+00:00"><meta itemprop=wordCount content="366"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width:720px;--font-main:Verdana, sans-serif;--font-secondary:Verdana, sans-serif;--font-scale:1em;--background-color:#fff;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--blockquote-color:#222}@media(prefers-color-scheme:dark){:root{--background-color:#01242e;--heading-color:#eee;--text-color:#ddd;--link-color:#8cc2dd;--visited-color:#8b6fcb;--blockquote-color:#ccc}}body{font-family:var(--font-secondary);font-size:var(--font-scale);margin:auto;padding:20px;max-width:var(--width);text-align:left;background-color:var(--background-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--text-color)}h1,h2,h3,h4,h5,h6{font-family:var(--font-main);color:var(--heading-color)}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}nav a{margin-right:8px}strong,b{color:var(--heading-color)}button{margin:0;cursor:pointer}time{font-family:monospace;font-style:normal;font-size:15px}main{line-height:1.6}table{width:100%}hr{border:0;border-top:1px dashed}img{max-width:100%}code{font-family:monospace;padding:2px;border-radius:3px}blockquote{border-left:1px solid #999;color:var(--blockquote-color);padding-left:20px;font-style:italic}footer{padding:25px 0;text-align:center}.title:hover{text-decoration:none}.title h1{font-size:1.5em}.inline{width:auto!important}.highlight,.code{border-radius:3px;margin-block-start:1em;margin-block-end:1em;overflow-x:auto}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:var(--visited-color)}</style><style type=text/css>@media(prefers-color-scheme:dark){body{background-color:#1a2638}code{background-color:#222;color:#eba613}}a{text-decoration:none}a:hover{text-decoration:underline}ol li{counter-increment:listCounter}ol li::marker{content:'(' counter(listCounter)").  "}figure{text-align:center;font-style:italic;font-size:smaller;text-indent:0;display:block;margin-left:auto;margin-right:auto;max-width:75%}figure.left{float:left;margin-right:2em;max-width:75%}figure.right{float:right;margin-left:2em;max-width:75%}@media(max-width:800px){figure.left,figure.right{float:none;margin-top:2em;margin-bottom:2em;margin-right:2em;margin-left:2em;max-width:50%}}.common-footer{margin-top:1em;padding-top:1em}.content{margin-top:1em}code.has-jax{-webkit-font-smoothing:antialiased;background:inherit!important;border:none!important;font-size:100%}</style><meta property="og:type" content="website"><meta property="og:url" content><meta property="og:title" content><meta property="og:description" content><meta property="og:image" content><meta name=description content="Information Theory and Statistics Reading Group
This group met from Fall 2018 to Winter 2019.
Winter Quarter

[03/05/2019] Tong Yitang presents on Lecture 71. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation.
[02/26/2019] David Weber presents on Lecture 61. Topics include graphical models.
[02/19/2019] David Weber and Dmitry Shemetov present on Lecture 61. Topics include mutual information estimators and applications of mutual information to machine learning.
[02/12/2019] Dmitry Shemetov presents on Lecture 4 and 51. Topics include properties of submodular functions, maximizing submodular funcions, and an information theoretic clustering algorithm2.
[01/29/2019] Dmitry Shemetov presents on Lecture 31. Topics include the data processing inequality, submodular functions, submodularity of mutual information, maximizing submodular functions, and an application of sensor placement3.
[01/22/2019] David Weber presents on the challenges of estimating mutual information4 5.
[01/15/2019] Cong Xu presents on “Chapter 5: Data Compression”6.

Fall Quarter

[12/13/2018] Dmitry Shemetov presents an introduction to some work by Katalin Marton7: the blowing up lemma8, single-letter characterization, Ornstein’s copying lemma, and Talagrand’s theorem.
[12/07/2018] David Weber lays the foundations with “Chapter 8: Differential Entropy”6 and then presents on “Estimating Mutual Information”4.
[11/29/2018] Yiqun Shao presents9.
[11/09/2018] Stephen Sheng presents10.
[11/02/2018] Group read of “Chapter 11: Information Theory and Statistics”6.
[10/26/2018] Group read of “Chapter 6: Gambling and Data Compression”6.
[10/19/2018] Dmitry Shemetov presents on “Chapter 2: Lower bounds on minimax risk”11.
[10/12/2018] A problem solving session dedicated to Chapter 26.
[10/05/2018] Dmitry Shemetov presents the foundational material from “Chapter 2: Entropy, Relative Entropy, Mutual Information”6.
[9/28/2018] Organizing meeting.

References




A. Singh, “Data Processing and Information Theory Course Notes”&#160;&#8617;&#xfe0e;&#160;&#8617;&#xfe0e;&#160;&#8617;&#xfe0e;&#160;&#8617;&#xfe0e;&#160;&#8617;&#xfe0e;"></head><body><header><a href=/ class=title><h2>Dmitry's Very Online Internet Webpage Site</h2></a><nav><a href=/>Home</a>
<a href=/now/>Now</a>
<a href=/projects/>Projects</a>
<a href=/blog/>Blog</a></nav></header><main><content><h1 id=information-theory-and-statistics-reading-group>Information Theory and Statistics Reading Group</h1><p>This group met from Fall 2018 to Winter 2019.</p><h2 id=winter-quarter>Winter Quarter</h2><ul><li>[03/05/2019] Tong Yitang presents on Lecture 7<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation.</li><li>[02/26/2019] David Weber presents on Lecture 6<sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Topics include graphical models.</li><li>[02/19/2019] David Weber and Dmitry Shemetov present on Lecture 6<sup id=fnref2:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Topics include mutual information estimators and applications of mutual information to machine learning.</li><li>[02/12/2019] Dmitry Shemetov presents on Lecture 4 and 5<sup id=fnref3:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Topics include properties of submodular functions, maximizing submodular funcions, and an information theoretic clustering algorithm<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</li><li>[01/29/2019] Dmitry Shemetov presents on Lecture 3<sup id=fnref4:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Topics include the data processing inequality, submodular functions, submodularity of mutual information, maximizing submodular functions, and an application of sensor placement<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</li><li>[01/22/2019] David Weber presents on the challenges of estimating mutual information<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>.</li><li>[01/15/2019] Cong Xu presents on “Chapter 5: Data Compression”<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</li></ul><h2 id=fall-quarter>Fall Quarter</h2><ul><li>[12/13/2018] Dmitry Shemetov presents an introduction to some work by Katalin Marton<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>: the blowing up lemma<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>, single-letter characterization, Ornstein’s copying lemma, and Talagrand’s theorem.</li><li>[12/07/2018] David Weber lays the foundations with “Chapter 8: Differential Entropy”<sup id=fnref1:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> and then presents on “Estimating Mutual Information”<sup id=fnref1:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</li><li>[11/29/2018] Yiqun Shao presents<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>.</li><li>[11/09/2018] Stephen Sheng presents<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>.</li><li>[11/02/2018] Group read of “Chapter 11: Information Theory and Statistics”<sup id=fnref2:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</li><li>[10/26/2018] Group read of “Chapter 6: Gambling and Data Compression”<sup id=fnref3:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</li><li>[10/19/2018] Dmitry Shemetov presents on “Chapter 2: Lower bounds on minimax risk”<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>.</li><li>[10/12/2018] A problem solving session dedicated to Chapter 2<sup id=fnref4:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</li><li>[10/05/2018] Dmitry Shemetov presents the foundational material from “Chapter 2: Entropy, Relative Entropy, Mutual Information”<sup id=fnref5:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</li><li>[9/28/2018] Organizing meeting.</li></ul><h2 id=references>References</h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>A. Singh, <a href=https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lecs.html>“Data Processing and Information Theory Course Notes”</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref4:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>L. Faivishevsky, J. Goldberger, <a href=https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/faivishevsky.pdf>“A Nonparametric Information Theoretic Clustering Algorithm”</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>A. Krause, A. Singh, C. Guestrin, <a href=http://www.jmlr.org/papers/volume9/krause08a/krause08a.pdf>“Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies”</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>A. Kraskov, H. Stoegbauer, P. Grassberger, <a href=https://arxiv.org/abs/cond-mat/0305641>“Estimating Mutual Information”</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>J. Walters-Williams, Y. Li, <a href=https://link.springer.com/chapter/10.1007/978-3-642-02962-2_49>“Estimating Mutual Information: A Survey”</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>T. Cover, J. Thomas, <em>Elements of Information Theory</em> <a href=https://dshemetov.github.io/itsrg#fnref:1>↩</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref4:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref5:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>K. Marton, <a href=https://www.itsoc.org/resources/videos/isit-2013-istanbul/MartonISIT2013.pdf>“Distance Divergence Inequalities”</a>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>K. Marton, <a href=https://ieeexplore.ieee.org/document/1057176>“A simple proof of the blowing-up lemma”</a>&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>A. Gibbs, F. Su, <a href=https://arxiv.org/abs/math/0209021>“On choosing and bounding probability metrics”</a>&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>T. Naftali, N. Zagaslavsky, <a href=https://arxiv.org/abs/1503.02406>“Deep Learning and the Information Bottleneck Principle”</a>&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>A. Tsybakov, <em>Introduction to Nonparametric Estimation</em>&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></content><p></p></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>