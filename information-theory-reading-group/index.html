<!doctype html><html lang=en-us><head><meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>┐｜･ิω･ิ#｜┌ Dmitry's Online Webpage</title><meta name=title content="┐｜･ิω･ิ#｜┌ Dmitry's Online Webpage"><meta name=description content="Information Theory and Statistics Reading Group This group met from Fall 2018 to Winter 2019.
Winter Quarter [03/05/2019] Tong Yitang presents on Lecture 71. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation. [02/26/2019] David Weber presents on Lecture 61. Topics include graphical models. [02/19/2019] David Weber and Dmitry Shemetov present on Lecture 61. Topics include mutual information estimators and applications of mutual information to machine learning."><meta name=keywords content><meta property="og:title" content><meta property="og:description" content="Information Theory and Statistics Reading Group This group met from Fall 2018 to Winter 2019.
Winter Quarter [03/05/2019] Tong Yitang presents on Lecture 71. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation. [02/26/2019] David Weber presents on Lecture 61. Topics include graphical models. [02/19/2019] David Weber and Dmitry Shemetov present on Lecture 61. Topics include mutual information estimators and applications of mutual information to machine learning."><meta property="og:type" content="article"><meta property="og:url" content="https://dshemetov.github.io/information-theory-reading-group/"><meta property="og:image" content="https://dshemetov.github.io/images/math-dmitry.jpg"><meta property="article:section" content><meta property="article:published_time" content="2019-08-25T00:00:00+00:00"><meta property="article:modified_time" content="2019-08-25T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dshemetov.github.io/images/math-dmitry.jpg"><meta name=twitter:title content><meta name=twitter:description content="Information Theory and Statistics Reading Group This group met from Fall 2018 to Winter 2019.
Winter Quarter [03/05/2019] Tong Yitang presents on Lecture 71. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation. [02/26/2019] David Weber presents on Lecture 61. Topics include graphical models. [02/19/2019] David Weber and Dmitry Shemetov present on Lecture 61. Topics include mutual information estimators and applications of mutual information to machine learning."><meta itemprop=name content><meta itemprop=description content="Information Theory and Statistics Reading Group This group met from Fall 2018 to Winter 2019.
Winter Quarter [03/05/2019] Tong Yitang presents on Lecture 71. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation. [02/26/2019] David Weber presents on Lecture 61. Topics include graphical models. [02/19/2019] David Weber and Dmitry Shemetov present on Lecture 61. Topics include mutual information estimators and applications of mutual information to machine learning."><meta itemprop=datePublished content="2019-08-25T00:00:00+00:00"><meta itemprop=dateModified content="2019-08-25T00:00:00+00:00"><meta itemprop=wordCount content="366"><meta itemprop=image content="https://dshemetov.github.io/images/math-dmitry.jpg"><meta itemprop=keywords content><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style><script>MathJax={loader:{load:["[tex]/mathtools"]},tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,macros:{R:"{\\mathbb R}",Z:"{\\mathbb Z}",C:"{\\mathbb C}"},packages:{"[+]":["mathtools"]}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style type=text/css>@media(prefers-color-scheme:dark){body{background-color:#1a2638}code{background-color:#222;color:#eba613}}a{text-decoration:none}a:hover{text-decoration:underline}ol li{counter-increment:listCounter}ol li::marker{content:'(' counter(listCounter)").  "}figure{text-align:center;font-style:italic;font-size:smaller;text-indent:0;display:block;margin-left:auto;margin-right:auto;max-width:75%}figure.left{float:left;margin-right:2em;max-width:75%}figure.right{float:right;margin-left:2em;max-width:75%}@media(max-width:800px){figure.left,figure.right{float:none;margin-top:2em;margin-bottom:2em;margin-right:2em;margin-left:2em;max-width:50%}}.common-footer{margin-top:1em;padding-top:1em}.content{margin-top:1em}code.has-jax{-webkit-font-smoothing:antialiased;background:inherit!important;border:none!important;font-size:100%}</style><meta property="og:type" content="website"><meta property="og:url" content><meta property="og:title" content><meta property="og:description" content><meta property="og:image" content><meta name=description content="Information Theory and Statistics Reading Group This group met from Fall 2018 to Winter 2019.
Winter Quarter [03/05/2019] Tong Yitang presents on Lecture 71. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation. [02/26/2019] David Weber presents on Lecture 61. Topics include graphical models. [02/19/2019] David Weber and Dmitry Shemetov present on Lecture 61. Topics include mutual information estimators and applications of mutual information to machine learning."></head><body><header><a href=/ class=title><h2>┐｜･ิω･ิ#｜┌ Dmitry's Online Webpage</h2></a><nav><a href=/>Home</a>
<a href=/now/>Now</a>
<a href=/projects/>Projects</a>
<a href=/blog>Blog</a></nav></header><main><content><h1 id=information-theory-and-statistics-reading-group>Information Theory and Statistics Reading Group</h1><p>This group met from Fall 2018 to Winter 2019.</p><h2 id=winter-quarter>Winter Quarter</h2><ul><li>[03/05/2019] Tong Yitang presents on Lecture 7<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation.</li><li>[02/26/2019] David Weber presents on Lecture 6<sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Topics include graphical models.</li><li>[02/19/2019] David Weber and Dmitry Shemetov present on Lecture 6<sup id=fnref2:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Topics include mutual information estimators and applications of mutual information to machine learning.</li><li>[02/12/2019] Dmitry Shemetov presents on Lecture 4 and 5<sup id=fnref3:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Topics include properties of submodular functions, maximizing submodular funcions, and an information theoretic clustering algorithm<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</li><li>[01/29/2019] Dmitry Shemetov presents on Lecture 3<sup id=fnref4:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Topics include the data processing inequality, submodular functions, submodularity of mutual information, maximizing submodular functions, and an application of sensor placement<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</li><li>[01/22/2019] David Weber presents on the challenges of estimating mutual information<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>.</li><li>[01/15/2019] Cong Xu presents on “Chapter 5: Data Compression”<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</li></ul><h2 id=fall-quarter>Fall Quarter</h2><ul><li>[12/13/2018] Dmitry Shemetov presents an introduction to some work by Katalin Marton<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>: the blowing up lemma<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>, single-letter characterization, Ornstein’s copying lemma, and Talagrand’s theorem.</li><li>[12/07/2018] David Weber lays the foundations with “Chapter 8: Differential Entropy”<sup id=fnref1:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> and then presents on “Estimating Mutual Information”<sup id=fnref1:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</li><li>[11/29/2018] Yiqun Shao presents<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>.</li><li>[11/09/2018] Stephen Sheng presents<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>.</li><li>[11/02/2018] Group read of “Chapter 11: Information Theory and Statistics”<sup id=fnref2:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</li><li>[10/26/2018] Group read of “Chapter 6: Gambling and Data Compression”<sup id=fnref3:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</li><li>[10/19/2018] Dmitry Shemetov presents on “Chapter 2: Lower bounds on minimax risk”<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>.</li><li>[10/12/2018] A problem solving session dedicated to Chapter 2<sup id=fnref4:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</li><li>[10/05/2018] Dmitry Shemetov presents the foundational material from “Chapter 2: Entropy, Relative Entropy, Mutual Information”<sup id=fnref5:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</li><li>[9/28/2018] Organizing meeting.</li></ul><h2 id=references>References</h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>A. Singh, <a href=https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lecs.html>“Data Processing and Information Theory Course Notes”</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref4:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>L. Faivishevsky, J. Goldberger, <a href=https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/faivishevsky.pdf>“A Nonparametric Information Theoretic Clustering Algorithm”</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>A. Krause, A. Singh, C. Guestrin, <a href=http://www.jmlr.org/papers/volume9/krause08a/krause08a.pdf>“Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies”</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>A. Kraskov, H. Stoegbauer, P. Grassberger, <a href=https://arxiv.org/abs/cond-mat/0305641>“Estimating Mutual Information”</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>J. Walters-Williams, Y. Li, <a href=https://link.springer.com/chapter/10.1007/978-3-642-02962-2_49>“Estimating Mutual Information: A Survey”</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>T. Cover, J. Thomas, <em>Elements of Information Theory</em> <a href=https://dshemetov.github.io/itsrg#fnref:1>↩</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref4:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref5:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>K. Marton, <a href=https://www.itsoc.org/resources/videos/isit-2013-istanbul/MartonISIT2013.pdf>“Distance Divergence Inequalities”</a>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>K. Marton, <a href=https://ieeexplore.ieee.org/document/1057176>“A simple proof of the blowing-up lemma”</a>&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>A. Gibbs, F. Su, <a href=https://arxiv.org/abs/math/0209021>“On choosing and bounding probability metrics”</a>&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>T. Naftali, N. Zagaslavsky, <a href=https://arxiv.org/abs/1503.02406>“Deep Learning and the Information Bottleneck Principle”</a>&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>A. Tsybakov, <em>Introduction to Nonparametric Estimation</em>&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></content><p></p></main><footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>