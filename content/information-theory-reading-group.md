---
title: "Information Theory and Statistics Reading Group"
date: 2019-08-25
---

# Information Theory and Statistics Reading Group

This group met from Fall 2018 to Winter 2019.

## Winter Quarter

- [03/05/2019] Tong Yitang presents on Lecture 7[^1]. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation.
- [02/26/2019] David Weber presents on Lecture 6[^1]. Topics include graphical models.
- [02/19/2019] David Weber and Dmitry Shemetov present on Lecture 6[^1]. Topics include mutual information estimators and applications of mutual information to machine learning.
- [02/12/2019] Dmitry Shemetov presents on Lecture 4 and 5[^1]. Topics include properties of submodular functions, maximizing submodular funcions, and an information theoretic clustering algorithm[^2].
- [01/29/2019] Dmitry Shemetov presents on Lecture 3[^1]. Topics include the data processing inequality, submodular functions, submodularity of mutual information, maximizing submodular functions, and an application of sensor placement[^3].
- [01/22/2019] David Weber presents on the challenges of estimating mutual information[^4] [^5].
- [01/15/2019] Cong Xu presents on “Chapter 5: Data Compression”[^6].

## Fall Quarter

- [12/13/2018] Dmitry Shemetov presents an introduction to some work by Katalin Marton[^7]: the blowing up lemma[^8], single-letter characterization, Ornstein’s copying lemma, and Talagrand’s theorem.
- [12/07/2018] David Weber lays the foundations with “Chapter 8: Differential Entropy”[^6] and then presents on “Estimating Mutual Information”[^4].
- [11/29/2018] Yiqun Shao presents[^9].
- [11/09/2018] Stephen Sheng presents[^10].
- [11/02/2018] Group read of “Chapter 11: Information Theory and Statistics”[^6].
- [10/26/2018] Group read of “Chapter 6: Gambling and Data Compression”[^6].
- [10/19/2018] Dmitry Shemetov presents on “Chapter 2: Lower bounds on minimax risk”[^11].
- [10/12/2018] A problem solving session dedicated to Chapter 2[^6].
- [10/05/2018] Dmitry Shemetov presents the foundational material from “Chapter 2: Entropy, Relative Entropy, Mutual Information”[^6].
- [9/28/2018] Organizing meeting.

## References

[^1]: A. Singh, [“Data Processing and Information Theory Course Notes”](https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lecs.html)
[^2]: L. Faivishevsky, J. Goldberger, [“A Nonparametric Information Theoretic Clustering Algorithm”](https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/faivishevsky.pdf) 
[^3]: A. Krause, A. Singh, C. Guestrin, [“Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies”](http://www.jmlr.org/papers/volume9/krause08a/krause08a.pdf) 
[^4]: A. Kraskov, H. Stoegbauer, P. Grassberger, [“Estimating Mutual Information”](https://arxiv.org/abs/cond-mat/0305641)
[^5]: J. Walters-Williams, Y. Li, [“Estimating Mutual Information: A Survey”](https://link.springer.com/chapter/10.1007/978-3-642-02962-2_49)
[^6]: T. Cover, J. Thomas, _Elements of Information Theory_ [↩](https://dshemetov.github.io/itsrg#fnref:1)
[^7]: K. Marton, [“Distance Divergence Inequalities”](https://www.itsoc.org/resources/videos/isit-2013-istanbul/MartonISIT2013.pdf)
[^8]: K. Marton, [“A simple proof of the blowing-up lemma”](https://ieeexplore.ieee.org/document/1057176)
[^9]: A. Gibbs, F. Su, [“On choosing and bounding probability metrics”](https://arxiv.org/abs/math/0209021)
[^10]: T. Naftali, N. Zagaslavsky, [“Deep Learning and the Information Bottleneck Principle”](https://arxiv.org/abs/1503.02406)
[^11]: A. Tsybakov, _Introduction to Nonparametric Estimation_
